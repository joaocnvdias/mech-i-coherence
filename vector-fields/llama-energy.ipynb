{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65ce6827",
   "metadata": {},
   "source": [
    "Current questions on development: \n",
    "\n",
    "- Should we include the first (embedding) layer in energy computations? I don't want to leave out model information but it leads to NaNs in the padding tokens since they generate the same activations in this layer -> null vectors -> 0 dot product\n",
    "- Should we do an if case for when the norm of the vector is 0 in the angle computation? Or is NaN more approriate (right now I think the second, because having an angle of 0 has a meaning that in this case wouldn't be the same) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5887bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import json\n",
    "from math import acos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c914fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LLMfunctions import inference_activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42b58eb",
   "metadata": {},
   "source": [
    "## Recreate GPT-2XL set-up for Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d3b39f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_topic = 'viktor'\n",
    "prompt_sufix = '_' + prompt_topic\n",
    "with open('prompts-gen/'+prompt_topic+'.txt') as file:\n",
    "    prompt = file.read()\n",
    "prompt = json.loads(prompt, strict=False) #transform string to dict ready for model; strict ignores space characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aff6754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1471ee8c6fa43b8a11615368b687e95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "    \n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id, padding_side = \"left\") #choose where padding will be applioed\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id #required in llama because no padding token is defined\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16f9e338",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template(prompt, add_generation_prompt=True, tokenize=False) #prompt-adds token when the model should generate; tokenize- if we should tokenize the output, rn will be a string\n",
    "inputs = tokenizer(text, padding=\"longest\", return_tensors=\"pt\") #transform into pt (pytorch) tensors; pad to the longest sequence in the batch\n",
    "inputs = {key: val.cuda() for key, val in inputs.items()} #move inputs into cuda\n",
    "temp_texts=tokenizer.batch_decode(inputs[\"input_ids\"], skip_special_tokens=True) #way to debug inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d6ba567",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_generations = 5  \n",
    "\n",
    "generations = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=400,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=terminators,\n",
    "    num_return_sequences=num_generations  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "485e0a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)\n",
    "decoded_gens = tokenizer.batch_decode(generations, skip_special_tokens=True)\n",
    "decoded_stories = [tokens[len(prompt_text):] for tokens in decoded_gens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9a8991",
   "metadata": {},
   "source": [
    "Bugs I found:\n",
    "- We are passing through the pipeline the generation including the prompt. We need to find the number of tokens in the tensor corresponding to the prompt - this has to be done in the forward pass. - done "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d5a82b",
   "metadata": {},
   "source": [
    "## Fixing angle computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "87122867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#legacy pipepline\n",
    "\n",
    "def compute_layer_vectors(layer_activation):\n",
    "    return layer_activation[1:]-layer_activation[:-1] #matrix except first_row - matrix except last_row\n",
    "\n",
    "def compute_vectors(hidden_states):\n",
    "    return [compute_layer_vectors(layer) for layer in hidden_states]\n",
    "\n",
    "def compute_dot_product_layer(layer_vectors):\n",
    "    return torch.stack([torch.dot(layer_vectors[i,:],layer_vectors[i+1,:]) for i in range(layer_vectors.shape[0]-1)]) #these are 1D vectors so keep torch.dot\n",
    "\n",
    "def compute_dot_product(vector_transitions_trajectory):\n",
    "    return [compute_dot_product_layer(layer_vectors) for layer_vectors in vector_transitions_trajectory]  \n",
    "\n",
    "def average_layer_dot_product(layer_dot_product):\n",
    "    # return layer_dot_product.nanmean()\n",
    "    return layer_dot_product.mean()\n",
    "\n",
    "def average_dot_product(dot_product_list):\n",
    "    return torch.stack([average_layer_dot_product(layer_dot_product) for layer_dot_product in dot_product_list])\n",
    "\n",
    "def sum_layer_energy(average_layer_dot_product):\n",
    "    return average_layer_dot_product.sum()\n",
    "\n",
    "def energy_pipeline(layer_hidden_states):\n",
    "    if not isinstance(layer_hidden_states, list):\n",
    "        raise TypeError(\"Expected a list of tensors (one per layer + embedding layer).\")\n",
    "    return sum_layer_energy(average_dot_product(compute_dot_product(compute_vectors(layer_hidden_states)))).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "be5f92dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new pipeline\n",
    "\n",
    "def compute_layer_vectors(layer_activation):\n",
    "    return layer_activation[1:]-layer_activation[:-1] #matrix except first_row - matrix except last_row\n",
    "\n",
    "def compute_vectors(hidden_states):\n",
    "    return [compute_layer_vectors(layer) for layer in hidden_states]\n",
    "\n",
    "def compute_angle_layer(layer_vectors):\n",
    "    angles=[]\n",
    "\n",
    "    for i in range(layer_vectors.shape[0]-1):\n",
    "        a = layer_vectors[i,:]\n",
    "        b = layer_vectors[i+1,:]\n",
    "\n",
    "        # if torch.norm(a) == 0 or torch.norm(b) == 0. #doesnt really make sense to do this\n",
    "        #     angles.append(0)\n",
    "\n",
    "        angles.append(acos(torch.dot(a, b) / (torch.norm(a) * torch.norm(b))))\n",
    "\n",
    "    return torch.tensor(angles, dtype=torch.bfloat16)\n",
    "\n",
    "def compute_angle(vector_transitions_trajectory):\n",
    "    return [compute_angle_layer(layer_vectors) for layer_vectors in vector_transitions_trajectory]\n",
    "\n",
    "def average_layer_angle(layer_dot_product):\n",
    "    # return layer_dot_product.nanmean()\n",
    "    return layer_dot_product.nanmean()\n",
    "\n",
    "def average_angle(dot_product_list):\n",
    "    return torch.stack([average_layer_angle(layer_dot_product) for layer_dot_product in dot_product_list])\n",
    "\n",
    "def sum_layer_energy(average_layer_dot_product):\n",
    "    return average_layer_dot_product.sum()\n",
    "\n",
    "def energy_pipeline_angles(layer_hidden_states):\n",
    "    if not isinstance(layer_hidden_states, list):\n",
    "        raise TypeError(\"Expected a list of tensors (one per layer + embedding layer).\")\n",
    "    return sum_layer_energy(average_angle(compute_angle(compute_vectors(layer_hidden_states)))).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3a6e28fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_size_prompt = inputs['input_ids'].shape[1]\n",
    "first_gen_withtout_prompt = generations[0,tensor_size_prompt:].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a728d803",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_values = []\n",
    "for i in range(num_generations):\n",
    "    tensor = generations[i,tensor_size_prompt:].unsqueeze(0) #shape 1xseq_length; remove prompt tokens\n",
    "    activations = inference_activations(model,tensor)\n",
    "    energy_values.append(energy_pipeline_angles(activations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1e28c50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[60.5, 64.5, 65.0, 60.0, 59.5]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "energy_values #nan, nan, 65, nan, nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229518f8",
   "metadata": {},
   "source": [
    "#### Debugging pipeline - NaN energy values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de248876",
   "metadata": {},
   "source": [
    "**Bug**: N-1 stories in batch return NaN values.\n",
    "\n",
    "**Reason**: The last tokens in most of the stories correspond to the padding token, which leads to the same activations in the first layer (which job is to embed). In return, the vectors of consecutive positions are null which lead to NaNs when computing the angle between them.\n",
    "\n",
    "**Debugging task**: Make sure that the only story which doesn't return NaN is the largest (no padding) in the number of tokens. \n",
    "\n",
    "- **Hypothesis**: The largest story  is the only story that doesn't need padding at the end of the sentence so it doesn't repeat tokens. Therefore there's no vectors equal to zero which are the cause of NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "586c37d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The story at index 2 is the longest therefore it shouldn't include NaNs.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(generations)):\n",
    "    if generations[i,-2] != 128009: #padding token\n",
    "        print(f\"The story at index {i} is the longest therefore it shouldn't include NaNs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "32861d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story with index 0 has an energy value of nan\n",
      "Story with index 1 has an energy value of nan\n",
      "Story with index 2 has an energy value of 65.0\n",
      "Story with index 3 has an energy value of nan\n",
      "Story with index 4 has an energy value of nan\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(generations)):\n",
    "    print(f'Story with index {i} has an energy value of {energy_values[i]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coherence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
