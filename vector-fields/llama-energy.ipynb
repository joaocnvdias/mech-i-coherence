{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65ce6827",
   "metadata": {},
   "source": [
    "Current questions on development: \n",
    "\n",
    "\n",
    "- We are passing through the pipeline the generation including the prompt. We need to find the number of tokens in the tensor corresponding to the prompt. This should be removed in the forward pass **- DONE**\n",
    "- Should we include the first (embedding) layer in energy computations? I don't want to leave out model information but:\n",
    "    - It leads to NaNs in the padding tokens since they generate the same activations in this layer -> null vectors -> 0 dot product **- FIXED**\n",
    "    - Remove first layer from energy computations?\n",
    "- Should we do an if case for when the norm of the vector is 0 in the angle computation? Or is NaN more approriate (right now I think the second, because having an angle of 0 has a meaning that in this case wouldn't be the same) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e5887bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import json\n",
    "from math import acos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39c914fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LLMfunctions import inference_activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42b58eb",
   "metadata": {},
   "source": [
    "## Recreate GPT-2XL set-up for Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d3b39f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_topic = 'viktor'\n",
    "prompt_sufix = '_' + prompt_topic\n",
    "with open('prompts-gen/'+prompt_topic+'.txt') as file:\n",
    "    prompt = file.read()\n",
    "prompt = json.loads(prompt, strict=False) #transform string to dict ready for model; strict ignores space characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aff6754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d395d3a227a4e9785a352ff0717150b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "    \n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id, padding_side = \"left\") #choose where padding will be applied\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id #required in llama because no padding token is defined\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16f9e338",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template(prompt, add_generation_prompt=True, tokenize=False) #prompt-adds token when the model should generate; tokenize- if we should tokenize the output, rn will be a string\n",
    "inputs = tokenizer(text, padding=\"longest\", return_tensors=\"pt\") #transform into pt (pytorch) tensors; pad to the longest sequence in the batch\n",
    "inputs = {key: val.cuda() for key, val in inputs.items()} #move inputs into cuda\n",
    "temp_texts=tokenizer.batch_decode(inputs[\"input_ids\"], skip_special_tokens=True) #way to debug inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d6ba567",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m num_generations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[0;32m----> 3\u001b[0m generations \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mterminators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_generations\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/llama/coherence/lib/python3.8/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/llama/coherence/lib/python3.8/site-packages/transformers/generation/utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2212\u001b[0m     )\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2235\u001b[0m     )\n",
      "File \u001b[0;32m~/code/llama/coherence/lib/python3.8/site-packages/transformers/generation/utils.py:3249\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3247\u001b[0m     probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   3248\u001b[0m     \u001b[38;5;66;03m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001b[39;00m\n\u001b[0;32m-> 3249\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   3250\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3251\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_generations = 5\n",
    "\n",
    "generations = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=400,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=terminators,\n",
    "    num_return_sequences=num_generations  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "485e0a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)\n",
    "decoded_gens = tokenizer.batch_decode(generations, skip_special_tokens=True)\n",
    "decoded_stories = [tokens[len(prompt_text):] for tokens in decoded_gens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d5a82b",
   "metadata": {},
   "source": [
    "## Fixing angle computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "87122867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#legacy pipepline\n",
    "\n",
    "def compute_layer_vectors(layer_activation):\n",
    "    return layer_activation[1:]-layer_activation[:-1] #matrix except first_row - matrix except last_row\n",
    "\n",
    "def compute_vectors(hidden_states):\n",
    "    return [compute_layer_vectors(layer) for layer in hidden_states]\n",
    "\n",
    "def compute_dot_product_layer(layer_vectors):\n",
    "    return torch.stack([torch.dot(layer_vectors[i,:],layer_vectors[i+1,:]) for i in range(layer_vectors.shape[0]-1)]) #these are 1D vectors so keep torch.dot\n",
    "\n",
    "def compute_dot_product(vector_transitions_trajectory):\n",
    "    return [compute_dot_product_layer(layer_vectors) for layer_vectors in vector_transitions_trajectory]  \n",
    "\n",
    "def average_layer_dot_product(layer_dot_product):\n",
    "    # return layer_dot_product.nanmean()\n",
    "    return layer_dot_product.mean()\n",
    "\n",
    "def average_dot_product(dot_product_list):\n",
    "    return torch.stack([average_layer_dot_product(layer_dot_product) for layer_dot_product in dot_product_list])\n",
    "\n",
    "def sum_layer_energy(average_layer_dot_product):\n",
    "    return average_layer_dot_product.sum()\n",
    "\n",
    "def energy_pipeline(layer_hidden_states):\n",
    "    if not isinstance(layer_hidden_states, list):\n",
    "        raise TypeError(\"Expected a list of tensors (one per layer + embedding layer).\")\n",
    "    return sum_layer_energy(average_dot_product(compute_dot_product(compute_vectors(layer_hidden_states)))).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5f92dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new pipeline\n",
    "\n",
    "def compute_layer_vectors(layer_activation):\n",
    "    return layer_activation[1:]-layer_activation[:-1] #matrix except first_row - matrix except last_row\n",
    "\n",
    "def compute_vectors(hidden_states):\n",
    "    return [compute_layer_vectors(layer) for layer in hidden_states]\n",
    "\n",
    "def compute_angle_layer(layer_vectors):\n",
    "    angles=[]\n",
    "\n",
    "    for i in range(layer_vectors.shape[0]-1):\n",
    "        a = layer_vectors[i,:]\n",
    "        b = layer_vectors[i+1,:]\n",
    "\n",
    "        # if torch.norm(a) == 0 or torch.norm(b) == 0. #doesnt really make sense to do this\n",
    "        #     angles.append(0)\n",
    "\n",
    "        angles.append(acos(torch.dot(a, b) / (torch.norm(a) * torch.norm(b))))\n",
    "\n",
    "    return torch.tensor(angles, dtype=torch.bfloat16)\n",
    "\n",
    "def compute_angle(vector_transitions_trajectory):\n",
    "    return [compute_angle_layer(layer_vectors) for layer_vectors in vector_transitions_trajectory]\n",
    "\n",
    "def average_layer_angle(layer_dot_product):\n",
    "    # return layer_dot_product.nanmean()\n",
    "    return layer_dot_product.nanmean()\n",
    "\n",
    "def average_angle(dot_product_list):\n",
    "    return torch.stack([average_layer_angle(layer_dot_product) for layer_dot_product in dot_product_list])\n",
    "\n",
    "def sum_layer_energy(average_layer_dot_product):\n",
    "    return average_layer_dot_product.sum()\n",
    "\n",
    "def energy_pipeline_angles(layer_hidden_states):\n",
    "    if not isinstance(layer_hidden_states, list):\n",
    "        raise TypeError(\"Expected a list of tensors (one per layer + embedding layer).\")\n",
    "    return sum_layer_energy(average_angle(compute_angle(compute_vectors(layer_hidden_states)))).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a728d803",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_size_prompt = inputs['input_ids'].shape[1]\n",
    "energy_values = []\n",
    "for i in range(num_generations):\n",
    "    tensor = generations[i,tensor_size_prompt:].unsqueeze(0) #shape 1xseq_length; remove prompt tokens\n",
    "    activations = inference_activations(model,tensor)\n",
    "    energy_values.append(energy_pipeline_angles(activations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1e28c50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60.5, 64.5, 65.0, 60.0, 59.5]\n"
     ]
    }
   ],
   "source": [
    "print(energy_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229518f8",
   "metadata": {},
   "source": [
    "#### Debugging pipeline - NaN energy values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de248876",
   "metadata": {},
   "source": [
    "**Bug**: N-1 stories in batch return NaN values.\n",
    "\n",
    "**Reason**: The last tokens in most of the stories correspond to the padding token, which leads to the same activations in the first layer (which job is to embed). In return, the vectors of consecutive positions are null which lead to NaNs when computing the angle between them.\n",
    "\n",
    "**Debugging task**: Make sure that the only story which doesn't return NaN is the largest (no padding) in the number of tokens. \n",
    "\n",
    "- **Hypothesis**: The largest story  is the only story that doesn't need padding at the end of the sentence so it doesn't repeat tokens. Therefore there's no vectors equal to zero which are the cause of NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "586c37d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The story at index 2 is the longest therefore it shouldn't include NaNs.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(generations)):\n",
    "    if generations[i,-2] != 128009: #padding token\n",
    "        print(f\"The story at index {i} is the longest therefore it shouldn't include NaNs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "32861d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story with index 0 has an energy value of nan\n",
      "Story with index 1 has an energy value of nan\n",
      "Story with index 2 has an energy value of 65.0\n",
      "Story with index 3 has an energy value of nan\n",
      "Story with index 4 has an energy value of nan\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(generations)):\n",
    "    print(f'Story with index {i} has an energy value of {energy_values[i]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coherence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
