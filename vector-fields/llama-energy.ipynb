{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e5887bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39c914fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LLMfunctions import inference_activations\n",
    "from EnergyComputations import energy_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42b58eb",
   "metadata": {},
   "source": [
    "## Recreate GPT-2XL set-up for Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d3b39f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_topic = 'viktor'\n",
    "prompt_sufix = '_' + prompt_topic\n",
    "with open('prompts-gen/'+prompt_topic+'.txt') as file:\n",
    "    prompt = file.read()\n",
    "prompt = json.loads(prompt, strict=False) #transform string to dict ready for model; strict ignores space characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aff6754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1471ee8c6fa43b8a11615368b687e95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "    \n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id, padding_side = \"left\") #choose where padding will be applioed\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id #required in llama because no padding token is defined\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16f9e338",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template(prompt, add_generation_prompt=True, tokenize=False) #prompt-adds token when the model should generate; tokenize- if we should tokenize the output, rn will be a string\n",
    "inputs = tokenizer(text, padding=\"longest\", return_tensors=\"pt\") #transform into pt (pytorch) tensors; pad to the longest sequence in the batch\n",
    "inputs = {key: val.cuda() for key, val in inputs.items()} #move inputs into cuda\n",
    "temp_texts=tokenizer.batch_decode(inputs[\"input_ids\"], skip_special_tokens=True) #way to debug inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d6ba567",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_generations = 5  \n",
    "\n",
    "generations = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=400,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=terminators,\n",
    "    num_return_sequences=num_generations  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "485e0a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)\n",
    "decoded_gens = tokenizer.batch_decode(generations, skip_special_tokens=True)\n",
    "decoded_stories = [tokens[len(prompt_text):] for tokens in decoded_gens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9a8991",
   "metadata": {},
   "source": [
    "Bugs I found:\n",
    "- We are passing through the pipeline the generation including the prompt. We need to find the number of tokens in the tensor corresponding to the prompt - this has to be done in the forward pass. - done "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d5a82b",
   "metadata": {},
   "source": [
    "## Fixing angle computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87122867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_layer_vectors(layer_activation):\n",
    "    return layer_activation[1:]-layer_activation[:-1] #matrix except first_row - matrix except last_row\n",
    "\n",
    "def compute_vectors(hidden_states):\n",
    "    return [compute_layer_vectors(layer) for layer in hidden_states]\n",
    "\n",
    "def compute_dot_product_layer(layer_vectors):\n",
    "    return torch.stack([torch.dot(layer_vectors[i,:],layer_vectors[i+1,:]) for i in range(layer_vectors.shape[0]-1)]) #these are 1D vectors so keep torch.dot\n",
    "\n",
    "def compute_dot_product(vector_transitions_trajectory):\n",
    "    return [compute_dot_product_layer(layer_vectors) for layer_vectors in vector_transitions_trajectory]  \n",
    "\n",
    "def average_layer_dot_product(layer_dot_product):\n",
    "    return layer_dot_product.mean()\n",
    "\n",
    "def average_dot_product(dot_product_list):\n",
    "    return torch.stack([average_layer_dot_product(layer_dot_product) for layer_dot_product in dot_product_list])\n",
    "\n",
    "def sum_layer_energy(average_layer_dot_product):\n",
    "    return average_layer_dot_product.sum()\n",
    "\n",
    "#legacy pipepline\n",
    "def energy_pipeline(layer_hidden_states):\n",
    "    if not isinstance(layer_hidden_states, list):\n",
    "        raise TypeError(\"Expected a list of tensors (one per layer + embedding layer).\")\n",
    "    return sum_layer_energy(average_dot_product(compute_dot_product(compute_vectors(layer_hidden_states)))).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5f92dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import acos\n",
    "\n",
    "def compute_angle_layer(layer_vectors):\n",
    "    angles=[]\n",
    "\n",
    "    for i in range(layer_vectors.shape[0]-1):\n",
    "        a = layer_vectors[i,:]\n",
    "        b = layer_vectors[i+1,:]\n",
    "        angles.append(acos(torch.dot(a, b) / (torch.norm(a) * torch.norm(b))))\n",
    "\n",
    "    return torch.tensor(angles, dtype=torch.bfloat16)\n",
    "\n",
    "def compute_angle(vector_transitions_trajectory):\n",
    "    return [compute_angle_layer(layer_vectors) for layer_vectors in vector_transitions_trajectory]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ebcd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_pipeline(layer_hidden_states):\n",
    "    if not isinstance(layer_hidden_states, list):\n",
    "        raise TypeError(\"Expected a list of tensors (one per layer + embedding layer).\")\n",
    "    return sum_layer_energy(average_dot_product(compute_angle(compute_vectors(layer_hidden_states)))).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6e28fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_size_prompt = inputs['input_ids'].shape[1]\n",
    "first_gen_withtout_prompt = generations[0,tensor_size_prompt:].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a728d803",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_values = []\n",
    "for i in range(num_generations):\n",
    "    tensor = generations[i,tensor_size_prompt:].unsqueeze(0) #shape 1xseq_length\n",
    "    print(f'Tensor: {tensor} \\n Tensor shape: {tensor.shape}')\n",
    "    # activations = inference_activations(model,tensor)\n",
    "    # energy_values.append(energy_pipeline(activations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73252e4d",
   "metadata": {},
   "source": [
    "##### Debugging with claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12937d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the actual input to see the structure\n",
    "input_text = tokenizer.decode(generations[0], skip_special_tokens=False)\n",
    "print(\"Full input sequence:\")\n",
    "print(input_text)\n",
    "\n",
    "# Check the first several tokens\n",
    "print(\"\\nFirst 10 tokens:\")\n",
    "for i in range(min(10, len(generations[0]))):\n",
    "    token_id = generations[0][i].item()\n",
    "    token_text = tokenizer.decode([token_id])\n",
    "    print(f\"Position {i}: {token_id} -> '{token_text}'\")\n",
    "\n",
    "# Look for where the actual content starts\n",
    "print(f\"\\nSpecial tokens:\")\n",
    "print(f\"BOS token: {tokenizer.bos_token_id}\")\n",
    "print(f\"EOS token: {tokenizer.eos_token_id}\")\n",
    "print(f\"PAD token: {tokenizer.pad_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5300f6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### --------- \n",
    "\n",
    "# Skip the initial special tokens and find content tokens that repeat\n",
    "content_start = 0\n",
    "for i, token in enumerate(generations[0]):\n",
    "    if token not in [tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.pad_token_id, 128000, 128006, 128007, 128009]:\n",
    "        content_start = i\n",
    "        break\n",
    "\n",
    "print(f\"Content starts at position: {content_start}\")\n",
    "\n",
    "# Find repeated content tokens (not special tokens)\n",
    "content_tokens = generations[0][content_start:]\n",
    "repeated_positions = {}\n",
    "for i, token in enumerate(content_tokens):\n",
    "    token_id = token.item()\n",
    "    if token_id not in repeated_positions:\n",
    "        repeated_positions[token_id] = []\n",
    "    repeated_positions[token_id].append(i + content_start)\n",
    "\n",
    "# Find tokens that appear multiple times in content\n",
    "repeated_content_tokens = {k: v for k, v in repeated_positions.items() if len(v) > 1}\n",
    "\n",
    "print(\"Repeated content tokens:\")\n",
    "for token_id, positions in repeated_content_tokens.items():\n",
    "    token_text = tokenizer.decode([token_id])\n",
    "    print(f\"Token {token_id} ('{token_text}'): positions {positions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c08a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## --------------- \n",
    "\n",
    "# Get the original input length\n",
    "original_input_length = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "# Extract just the generated tokens for each sequence\n",
    "generated_tokens_only = []\n",
    "for i in range(num_generations):\n",
    "    # generations[i] contains: [original_input + new_generated_tokens]\n",
    "    generated_part = generations[i][original_input_length:]\n",
    "    generated_tokens_only.append(generated_part)\n",
    "\n",
    "print(f\"Original input length: {original_input_length}\")\n",
    "print(f\"Full generation length: {len(generations[0])}\")\n",
    "print(f\"Generated tokens only length: {len(generated_tokens_only[0])}\")\n",
    "\n",
    "# Decode to see what was actually generated\n",
    "print(\"\\nGenerated text only:\")\n",
    "for i, gen_tokens in enumerate(generated_tokens_only):\n",
    "    gen_text = tokenizer.decode(gen_tokens, skip_special_tokens=True)\n",
    "    print(f\"Generation {i}: {gen_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6078445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----------------- \n",
    "\n",
    "# Use the generated tokens for your hidden state analysis\n",
    "with torch.no_grad():\n",
    "    # Take first generation's new tokens only\n",
    "    gen_tokens = generated_tokens_only[0].unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    full_outputs = model(\n",
    "        input_ids=gen_tokens,\n",
    "        output_hidden_states=True,\n",
    "        return_dict=True\n",
    "    )\n",
    "\n",
    "# Remove 1st tensor dimension so it's 2D\n",
    "hidden_states = [layer[0] for layer in full_outputs.hidden_states]\n",
    "\n",
    "# Now find repeated tokens in the generated content only\n",
    "gen_tokens_flat = generated_tokens_only[0].cpu().numpy()\n",
    "repeated_positions = {}\n",
    "for i, token in enumerate(gen_tokens_flat):\n",
    "    if token not in repeated_positions:\n",
    "        repeated_positions[token] = []\n",
    "    repeated_positions[token].append(i)\n",
    "\n",
    "# Find tokens that appear multiple times\n",
    "repeated_tokens = {k: v for k, v in repeated_positions.items() if len(v) > 1}\n",
    "\n",
    "print(\"Repeated tokens in generated content:\")\n",
    "for token_id, positions in repeated_tokens.items():\n",
    "    token_text = tokenizer.decode([token_id])\n",
    "    print(f\"Token {token_id} ('{token_text}'): positions {positions}\")\n",
    "\n",
    "# Test your hypothesis on these repeated tokens\n",
    "if repeated_tokens:\n",
    "    for token_id, positions in list(repeated_tokens.items())[:3]:  # Test first 3\n",
    "        if len(positions) >= 2:\n",
    "            pos1, pos2 = positions[0], positions[1]\n",
    "            token_text = tokenizer.decode([token_id])\n",
    "            print(f\"\\nComparing token '{token_text}' at positions {pos1} and {pos2}:\")\n",
    "            \n",
    "            for layer_idx in [0, 8, 16, 24, 31]:\n",
    "                h1 = hidden_states[layer_idx][pos1]\n",
    "                h2 = hidden_states[layer_idx][pos2]\n",
    "                \n",
    "                cos_sim = F.cosine_similarity(h1.unsqueeze(0), h2.unsqueeze(0))\n",
    "                l2_dist = torch.norm(h1 - h2)\n",
    "                \n",
    "                print(f\"Layer {layer_idx:2d}: Cosine similarity = {cos_sim.item():.4f}, L2 distance = {l2_dist.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coherence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
