{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bb2f1f0",
   "metadata": {},
   "source": [
    "This notebook was created with the intent of developing the rank centrality algorithm for pairwise data, but after investing some time into it, I found out that it was no longer needed and stop its development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132d6e6c",
   "metadata": {},
   "source": [
    "## Rank Centrality Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80bf6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joaodias/miniforge3/envs/coherence/lib/python3.9/site-packages/transformers/utils/hub.py:527: FutureWarning: Using `from_pretrained` with the url of a file (here https://storage.googleapis.com/sgnlp-models/models/coherence_momentum/config.json) is deprecated and won't be possible anymore in v5 of Transformers. You should host your file on the Hub (hf.co) instead and use the repository ID. Note that this is not compatible with the caching system (your file will be downloaded at each execution) or multiple processes (each process will download the file in a different temporary file).\n",
      "  warnings.warn(\n",
      "/Users/joaodias/miniforge3/envs/coherence/lib/python3.9/site-packages/transformers/utils/hub.py:527: FutureWarning: Using `from_pretrained` with the url of a file (here https://storage.googleapis.com/sgnlp-models/models/coherence_momentum/pytorch_model.bin) is deprecated and won't be possible anymore in v5 of Transformers. You should host your file on the Hub (hf.co) instead and use the repository ID. Note that this is not compatible with the caching system (your file will be downloaded at each execution) or multiple processes (each process will download the file in a different temporary file).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sgnlp.models.coherence_momentum import CoherenceMomentumModel, CoherenceMomentumConfig, \\\n",
    "    CoherenceMomentumPreprocessor\n",
    "\n",
    "# Load Model\n",
    "config = CoherenceMomentumConfig.from_pretrained(\n",
    "    \"https://storage.googleapis.com/sgnlp-models/models/coherence_momentum/config.json\"\n",
    ")\n",
    "model = CoherenceMomentumModel.from_pretrained(\n",
    "    \"https://storage.googleapis.com/sgnlp-models/models/coherence_momentum/pytorch_model.bin\",\n",
    "    config=config\n",
    ")\n",
    "\n",
    "preprocessor = CoherenceMomentumPreprocessor(config.model_size, config.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558b363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "random.seed(10)\n",
    "batch = random.sample(tiny_train['text'],1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090a7378",
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_flattened = [''.join(item) for item in positives]\n",
    "negatives_flattened = [''.join(item) for item in negatives]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b7e42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently at 0%\n",
      "Currently at 1%\n",
      "Currently at 2%\n",
      "Currently at 3%\n",
      "Currently at 4%\n",
      "Currently at 5%\n",
      "Currently at 6%\n",
      "Currently at 7%\n",
      "Currently at 8%\n",
      "Currently at 9%\n",
      "Currently at 10%\n",
      "Currently at 11%\n",
      "Currently at 11%\n",
      "Currently at 12%\n",
      "Currently at 13%\n",
      "Currently at 14%\n",
      "Currently at 15%\n",
      "Currently at 16%\n",
      "Currently at 17%\n",
      "Currently at 18%\n",
      "Currently at 19%\n",
      "Currently at 20%\n",
      "Currently at 21%\n",
      "Currently at 22%\n",
      "Currently at 23%\n",
      "Currently at 24%\n",
      "Currently at 25%\n",
      "Currently at 26%\n",
      "Currently at 27%\n",
      "Currently at 28%\n",
      "Currently at 29%\n",
      "Currently at 30%\n",
      "Currently at 31%\n",
      "Currently at 32%\n",
      "Currently at 33%\n",
      "Currently at 33%\n",
      "Currently at 34%\n",
      "Currently at 35%\n",
      "Currently at 36%\n",
      "Currently at 37%\n",
      "Currently at 38%\n",
      "Currently at 39%\n",
      "Currently at 40%\n",
      "Currently at 41%\n",
      "Currently at 42%\n",
      "Currently at 43%\n",
      "Currently at 44%\n",
      "Currently at 45%\n",
      "Currently at 46%\n",
      "Currently at 47%\n",
      "Currently at 48%\n",
      "Currently at 49%\n",
      "Currently at 50%\n",
      "Currently at 51%\n",
      "Currently at 52%\n",
      "Currently at 53%\n",
      "Currently at 54%\n",
      "Currently at 54%\n",
      "Currently at 55%\n",
      "Currently at 56%\n",
      "Currently at 57%\n",
      "Currently at 58%\n",
      "Currently at 59%\n",
      "Currently at 60%\n",
      "Currently at 61%\n",
      "Currently at 62%\n",
      "Currently at 63%\n",
      "Currently at 64%\n",
      "Currently at 65%\n",
      "Currently at 66%\n",
      "Currently at 67%\n",
      "Currently at 68%\n",
      "Currently at 69%\n",
      "Currently at 70%\n",
      "Currently at 71%\n",
      "Currently at 72%\n",
      "Currently at 73%\n",
      "Currently at 74%\n",
      "Currently at 75%\n",
      "Currently at 76%\n",
      "Currently at 76%\n",
      "Currently at 77%\n",
      "Currently at 78%\n",
      "Currently at 79%\n",
      "Currently at 80%\n",
      "Currently at 81%\n",
      "Currently at 82%\n",
      "Currently at 83%\n",
      "Currently at 84%\n",
      "Currently at 85%\n",
      "Currently at 86%\n",
      "Currently at 87%\n",
      "Currently at 88%\n",
      "Currently at 89%\n",
      "Currently at 90%\n",
      "Currently at 91%\n",
      "Currently at 92%\n",
      "Currently at 93%\n",
      "Currently at 94%\n",
      "Currently at 95%\n",
      "Currently at 96%\n",
      "Currently at 97%\n",
      "Currently at 98%\n",
      "Currently at 98%\n",
      "Currently at 99%\n"
     ]
    }
   ],
   "source": [
    "positive_scores = []\n",
    "for i,story in enumerate(positives_flattened):\n",
    "    tokenized = preprocessor([story])\n",
    "    score = model.get_main_score(tokenized[\"tokenized_texts\"]).item()\n",
    "    positive_scores.append(score)\n",
    "    if i%10 == 0:\n",
    "        print(f'Currently at {(i/len(positives_flattened)):.0%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a88ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently at 0%\n",
      "Currently at 1%\n",
      "Currently at 2%\n",
      "Currently at 3%\n",
      "Currently at 4%\n",
      "Currently at 5%\n",
      "Currently at 6%\n",
      "Currently at 7%\n",
      "Currently at 8%\n",
      "Currently at 9%\n",
      "Currently at 10%\n",
      "Currently at 11%\n",
      "Currently at 11%\n",
      "Currently at 12%\n",
      "Currently at 13%\n",
      "Currently at 14%\n",
      "Currently at 15%\n",
      "Currently at 16%\n",
      "Currently at 17%\n",
      "Currently at 18%\n",
      "Currently at 19%\n",
      "Currently at 20%\n",
      "Currently at 21%\n",
      "Currently at 22%\n",
      "Currently at 23%\n",
      "Currently at 24%\n",
      "Currently at 25%\n",
      "Currently at 26%\n",
      "Currently at 27%\n",
      "Currently at 28%\n",
      "Currently at 29%\n",
      "Currently at 30%\n",
      "Currently at 31%\n",
      "Currently at 32%\n",
      "Currently at 33%\n",
      "Currently at 33%\n",
      "Currently at 34%\n",
      "Currently at 35%\n",
      "Currently at 36%\n",
      "Currently at 37%\n",
      "Currently at 38%\n",
      "Currently at 39%\n",
      "Currently at 40%\n",
      "Currently at 41%\n",
      "Currently at 42%\n",
      "Currently at 43%\n",
      "Currently at 44%\n",
      "Currently at 45%\n",
      "Currently at 46%\n",
      "Currently at 47%\n",
      "Currently at 48%\n",
      "Currently at 49%\n",
      "Currently at 50%\n",
      "Currently at 51%\n",
      "Currently at 52%\n",
      "Currently at 53%\n",
      "Currently at 54%\n",
      "Currently at 54%\n",
      "Currently at 55%\n",
      "Currently at 56%\n",
      "Currently at 57%\n",
      "Currently at 58%\n",
      "Currently at 59%\n",
      "Currently at 60%\n",
      "Currently at 61%\n",
      "Currently at 62%\n",
      "Currently at 63%\n",
      "Currently at 64%\n",
      "Currently at 65%\n",
      "Currently at 66%\n",
      "Currently at 67%\n",
      "Currently at 68%\n",
      "Currently at 69%\n",
      "Currently at 70%\n",
      "Currently at 71%\n",
      "Currently at 72%\n",
      "Currently at 73%\n",
      "Currently at 74%\n",
      "Currently at 75%\n",
      "Currently at 76%\n",
      "Currently at 76%\n",
      "Currently at 77%\n",
      "Currently at 78%\n",
      "Currently at 79%\n",
      "Currently at 80%\n",
      "Currently at 81%\n",
      "Currently at 82%\n",
      "Currently at 83%\n",
      "Currently at 84%\n",
      "Currently at 85%\n",
      "Currently at 86%\n",
      "Currently at 87%\n",
      "Currently at 88%\n",
      "Currently at 89%\n",
      "Currently at 90%\n",
      "Currently at 91%\n",
      "Currently at 92%\n",
      "Currently at 93%\n",
      "Currently at 94%\n",
      "Currently at 95%\n",
      "Currently at 96%\n",
      "Currently at 97%\n",
      "Currently at 98%\n",
      "Currently at 98%\n",
      "Currently at 99%\n"
     ]
    }
   ],
   "source": [
    "negative_scores = []\n",
    "for i,story in enumerate(negatives_flattened):\n",
    "    tokenized = preprocessor([story])\n",
    "    score = model.get_main_score(tokenized[\"tokenized_texts\"]).item()\n",
    "    negative_scores.append(score)\n",
    "    if i%10 == 0:\n",
    "        print(f'Currently at {(i/len(negatives_flattened)):.0%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7104a449",
   "metadata": {},
   "outputs": [],
   "source": [
    "same = 0\n",
    "draw = 0\n",
    "\n",
    "winner = 999\n",
    "\n",
    "ant_J = []\n",
    "for idx in range(len(positive_scores)):\n",
    "    positive_rating = positive_scores[idx]\n",
    "    negative_rating = negative_scores[idx]\n",
    "    ground_truth = int(ant[idx])\n",
    "\n",
    "\n",
    "    if positive_rating>negative_rating:\n",
    "        winner = 0 #its weird but positive = 0 (i think)\n",
    "        ant_J.append(winner)\n",
    "    elif positive_rating<negative_rating:\n",
    "        winner = 1\n",
    "        ant_J.append(winner)\n",
    "    else:\n",
    "        draw+=1\n",
    "        ant_J.append(np.nan)\n",
    "    \n",
    "    if winner == ground_truth:\n",
    "        same+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c20249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Krippendorff's alpha: 0.0968\n"
     ]
    }
   ],
   "source": [
    "#krippendorff alpha\n",
    "import krippendorff\n",
    "\n",
    "#each row is a coder, each column is a unit/item\n",
    "data = np.array([\n",
    "    ant,\n",
    "    ant_J\n",
    "])\n",
    "\n",
    "alpha = krippendorff.alpha(reliability_data=data)\n",
    "print(f\"Krippendorff's alpha: {alpha:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeb6869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently at 0%\n",
      "Currently at 1%\n",
      "Currently at 2%\n",
      "Currently at 3%\n",
      "Currently at 4%\n",
      "Currently at 5%\n",
      "Currently at 6%\n",
      "Currently at 7%\n",
      "Currently at 8%\n",
      "Currently at 9%\n",
      "Currently at 10%\n",
      "Currently at 11%\n",
      "Currently at 12%\n",
      "Currently at 13%\n",
      "Currently at 14%\n",
      "Currently at 15%\n",
      "Currently at 16%\n",
      "Currently at 17%\n",
      "Currently at 18%\n",
      "Currently at 19%\n",
      "Currently at 20%\n",
      "Currently at 21%\n",
      "Currently at 22%\n",
      "Currently at 23%\n",
      "Currently at 24%\n",
      "Currently at 25%\n",
      "Currently at 26%\n",
      "Currently at 27%\n",
      "Currently at 28%\n",
      "Currently at 29%\n",
      "Currently at 30%\n",
      "Currently at 31%\n",
      "Currently at 32%\n",
      "Currently at 33%\n",
      "Currently at 34%\n",
      "Currently at 35%\n",
      "Currently at 36%\n",
      "Currently at 37%\n",
      "Currently at 38%\n",
      "Currently at 39%\n",
      "Currently at 40%\n",
      "Currently at 41%\n",
      "Currently at 42%\n",
      "Currently at 43%\n",
      "Currently at 44%\n",
      "Currently at 45%\n",
      "Currently at 46%\n",
      "Currently at 47%\n",
      "Currently at 48%\n",
      "Currently at 49%\n",
      "Currently at 50%\n",
      "Currently at 51%\n",
      "Currently at 52%\n",
      "Currently at 53%\n",
      "Currently at 54%\n",
      "Currently at 55%\n",
      "Currently at 56%\n",
      "Currently at 57%\n",
      "Currently at 58%\n",
      "Currently at 59%\n",
      "Currently at 60%\n",
      "Currently at 61%\n",
      "Currently at 62%\n",
      "Currently at 63%\n",
      "Currently at 64%\n",
      "Currently at 65%\n",
      "Currently at 66%\n",
      "Currently at 67%\n",
      "Currently at 68%\n",
      "Currently at 69%\n",
      "Currently at 70%\n",
      "Currently at 71%\n",
      "Currently at 72%\n",
      "Currently at 73%\n",
      "Currently at 74%\n",
      "Currently at 75%\n",
      "Currently at 76%\n",
      "Currently at 77%\n",
      "Currently at 78%\n",
      "Currently at 79%\n",
      "Currently at 80%\n",
      "Currently at 81%\n",
      "Currently at 82%\n",
      "Currently at 83%\n",
      "Currently at 84%\n",
      "Currently at 85%\n",
      "Currently at 86%\n",
      "Currently at 87%\n",
      "Currently at 88%\n",
      "Currently at 89%\n",
      "Currently at 90%\n",
      "Currently at 91%\n",
      "Currently at 92%\n",
      "Currently at 93%\n",
      "Currently at 94%\n",
      "Currently at 95%\n",
      "Currently at 96%\n",
      "Currently at 97%\n",
      "Currently at 98%\n",
      "Currently at 99%\n"
     ]
    }
   ],
   "source": [
    "all_scores = []\n",
    "for i,story in enumerate(batch):\n",
    "    tokenized = preprocessor([story])\n",
    "    score = model.get_main_score(tokenized[\"tokenized_texts\"]).item()\n",
    "    all_scores.append(score)\n",
    "    if i%10 == 0:\n",
    "        print(f'Currently at {(i/len(batch)):.0%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb7015b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "nr_comparisons = 10\n",
    "\n",
    "#maybe vectorize to avoid nesting\n",
    "for i, base_rating in enumerate(all_scores):\n",
    "    #we need at least O(nlogn) comparisons approx... for 1000 elements->7000\n",
    "    comparing_ids = np.random.choice(1000,nr_comparisons) #question, does it make sense to not remove previously selected stories\n",
    "    \n",
    "    for comparing_id in comparing_ids:\n",
    "        comparing_rating = all_scores[comparing_id] \n",
    "\n",
    "        if base_rating>comparing_rating:\n",
    "            results.append((i,comparing_id,0)) #base, comparison in question, winner position\n",
    "        else: \n",
    "            results.append((i,comparing_id,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c3eff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create win-count matrix\n",
    "\n",
    "a = np.zeros((len(batch), len(batch)))\n",
    "\n",
    "for i,j,winner in results:\n",
    "    if winner == 1:\n",
    "        a[i,j] +=1 \n",
    "    elif winner == 0:\n",
    "        a[j,i] +=1\n",
    "    else:\n",
    "        raise Exception('Invalid value for winner, must be 0 or 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bda3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the graph connected? - True\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def is_connected_from_pairwise(pairwise_matrix):\n",
    "    n = pairwise_matrix.shape[0]\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(n))\n",
    "    \n",
    "    # Add edges based on pairwise comparison matrix\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            if pairwise_matrix[i, j] > 0 or pairwise_matrix[j, i] > 0:\n",
    "                G.add_edge(i, j)\n",
    "    \n",
    "    # Check if the graph is connected\n",
    "    return nx.is_connected(G), G\n",
    "\n",
    "connection,graph = is_connected_from_pairwise(a)\n",
    "print(f'Is the graph connected? - {connection}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b141a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create observation matrix A\n",
    "\n",
    "n = a.shape[0]\n",
    "A = np.zeros_like(a)\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if i == j: #ignore diagonal (always 0)\n",
    "            continue\n",
    "        total = a[i, j] + a[j, i] #if a story #1 samples #2 and then #2 laters also samples #1, will be highte than 1\n",
    "        \n",
    "        if total > 0:\n",
    "            A[i, j] = a[i, j] / total\n",
    "            A[j, i] = a[j, i] / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63672cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute dmax - to implement\n",
    "\n",
    "d_values = np.sum(A[:,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06819ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating transition matrix\n",
    "new_A = np.full_like(A,0)\n",
    "\n",
    "for i in range(A.shape[0]):\n",
    "    for j in range(A.shape[1]):\n",
    "        new_A[i,j] = A[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d129f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking scores: [0.00154447 0.00062324 0.00054938 0.00045485 0.00074162 0.00025319\n",
      " 0.00027127 0.00068465 0.00167321 0.00047419 0.00139638 0.00045537\n",
      " 0.00109788 0.00160132 0.00157135 0.00026585 0.0004174  0.00157995\n",
      " 0.00044205 0.00087165 0.00041277 0.00025319 0.00051357 0.00043484\n",
      " 0.00064365 0.00118921 0.00026839 0.00065965 0.00030128 0.00049702\n",
      " 0.00035568 0.00135453 0.00071588 0.00029645 0.00140856 0.00031736\n",
      " 0.00029302 0.00075594 0.00033035 0.00094754 0.00048286 0.00455263\n",
      " 0.002001   0.00037377 0.00223305 0.00034406 0.00069587 0.00039662\n",
      " 0.00140919 0.00026522 0.0007596  0.00041015 0.00063657 0.00066027\n",
      " 0.00651317 0.00061553 0.00134091 0.00030841 0.0011293  0.00025319\n",
      " 0.00049447 0.00192567 0.00091125 0.00032772 0.00035632 0.00030088\n",
      " 0.00129663 0.00027127 0.0011179  0.00050937 0.00025319 0.00044622\n",
      " 0.00164607 0.00040582 0.00025319 0.00030786 0.00044679 0.00226766\n",
      " 0.00596223 0.00049919 0.00043074 0.0002647  0.00025319 0.00151366\n",
      " 0.00037394 0.00037633 0.00147014 0.00039243 0.00040615 0.00026725\n",
      " 0.00039951 0.00164951 0.00049639 0.00058272 0.00092268 0.00044279\n",
      " 0.00055962 0.00049429 0.0003423  0.00031477 0.00322402 0.00061006\n",
      " 0.00053804 0.00287027 0.00037847 0.00129137 0.00035906 0.00047464\n",
      " 0.00028014 0.00025319 0.00259047 0.00083612 0.00683655 0.00032378\n",
      " 0.00033795 0.00048636 0.00032483 0.00078543 0.00035306 0.00156985\n",
      " 0.00032089 0.00041558 0.00038414 0.00025319 0.00155335 0.00113881\n",
      " 0.00030157 0.00059644 0.00076459 0.00031851 0.00031321 0.00081635\n",
      " 0.00264545 0.00213614 0.00041633 0.00037156 0.00363222 0.0002889\n",
      " 0.00058811 0.00046615 0.00077764 0.00051275 0.00036159 0.00110712\n",
      " 0.00196984 0.00046417 0.00033211 0.00064579 0.00040017 0.00028906\n",
      " 0.00035669 0.00107512 0.00153424 0.00085955 0.00033592 0.00069663\n",
      " 0.00051975 0.00044358 0.00205064 0.00025319 0.00025319 0.00059375\n",
      " 0.00062824 0.00060622 0.0018463  0.00031667 0.00044487 0.00048038\n",
      " 0.00027911 0.00062105 0.00026546 0.0003606  0.00032684 0.00032268\n",
      " 0.00041702 0.00060111 0.0004025  0.00061553 0.00026774 0.00066943\n",
      " 0.00033795 0.0002813  0.00088671 0.00037067 0.00049644 0.0008359\n",
      " 0.00232738 0.00462619 0.00042005 0.00046552 0.00166581 0.00444209\n",
      " 0.00114534 0.00031103 0.00038463 0.00050244 0.00038724 0.00054724\n",
      " 0.00049286 0.00140106 0.00035032 0.00093365 0.00029412 0.00034128\n",
      " 0.00067628 0.00237669 0.00030661 0.00092069 0.00026585 0.00029067\n",
      " 0.00065089 0.00030015 0.00029643 0.00030634 0.00042418 0.00033893\n",
      " 0.00440718 0.00061837 0.00050011 0.0028543  0.00360268 0.00142202\n",
      " 0.00034693 0.00030037 0.00201049 0.00185151 0.00096821 0.00029069\n",
      " 0.00039712 0.00270358 0.00173008 0.00131414 0.00063976 0.00029887\n",
      " 0.00047714 0.00025319 0.0003704  0.00156521 0.0007072  0.00035103\n",
      " 0.00124334 0.00200612 0.00068862 0.00046976 0.00041361 0.01025072\n",
      " 0.00033257 0.00161008 0.00131196 0.00052647 0.00037368 0.00039331\n",
      " 0.00505345 0.0013096  0.00026525 0.00231374 0.00047846 0.0002894\n",
      " 0.00039811 0.00056646 0.00083885 0.00054169 0.00076464 0.00045244\n",
      " 0.00029673 0.00035711 0.00327015 0.00034912 0.00041612 0.00033299\n",
      " 0.00056792 0.00072187 0.00032016 0.00028499 0.00037563 0.00038748\n",
      " 0.00061192 0.00076305 0.00029498 0.00035833 0.00096946 0.00039907\n",
      " 0.0004067  0.00056844 0.00028002 0.00057331 0.00037373 0.00033534\n",
      " 0.00038999 0.00057171 0.00077053 0.00081836 0.00052593 0.00030374\n",
      " 0.0004699  0.00059143 0.00046799 0.00192297 0.00057563 0.00042286\n",
      " 0.00082973 0.00042322 0.0003935  0.00063544 0.00033877 0.00052757\n",
      " 0.00041827 0.00040462 0.00115324 0.00340009 0.0003473  0.00027149\n",
      " 0.00060289 0.00065386 0.00121497 0.00087512 0.00079706 0.00094331\n",
      " 0.00061011 0.00030877 0.00025319 0.00107117 0.00048935 0.00038149\n",
      " 0.00076073 0.00036912 0.0005257  0.0005407  0.00025319 0.00053866\n",
      " 0.0006936  0.00054438 0.00049767 0.00034182 0.00034583 0.00031706\n",
      " 0.00247105 0.00043955 0.00057597 0.00098997 0.00075251 0.00025319\n",
      " 0.000406   0.00032327 0.00027729 0.00035044 0.00032651 0.01286069\n",
      " 0.00086253 0.00078498 0.00092519 0.00130365 0.00386647 0.0003355\n",
      " 0.00051985 0.00082615 0.00066424 0.00290855 0.00136589 0.00039937\n",
      " 0.0004157  0.00127334 0.00252508 0.00123674 0.00501109 0.00051052\n",
      " 0.00044672 0.0005128  0.00165819 0.00045506 0.00062605 0.00026651\n",
      " 0.00025319 0.00027344 0.00035346 0.00106327 0.00071427 0.00030894\n",
      " 0.00226433 0.00049047 0.00073521 0.00039371 0.00057161 0.00028612\n",
      " 0.00121996 0.00042092 0.00079985 0.00027991 0.00063149 0.00082163\n",
      " 0.00058738 0.00215011 0.0030269  0.00073987 0.0002642  0.00130113\n",
      " 0.00048842 0.00071028 0.00034653 0.00051132 0.00029187 0.01854953\n",
      " 0.00077681 0.00037744 0.00425832 0.00052887 0.00060083 0.00048518\n",
      " 0.00025319 0.00037919 0.00056326 0.00029457 0.00055122 0.00043019\n",
      " 0.00038058 0.00064084 0.00100813 0.00094683 0.00850593 0.00050829\n",
      " 0.00047173 0.00081374 0.00051054 0.00031631 0.00079392 0.00164085\n",
      " 0.00025319 0.00025319 0.0003975  0.00075247 0.00034711 0.00149156\n",
      " 0.00030928 0.00059845 0.00031456 0.00109855 0.00089387 0.00555505\n",
      " 0.00116593 0.00029719 0.00099724 0.00108265 0.00053053 0.00038898\n",
      " 0.00106002 0.00104539 0.00118696 0.00028367 0.00089013 0.00140149\n",
      " 0.00025319 0.00054896 0.00025319 0.00047769 0.00028948 0.00036843\n",
      " 0.00031977 0.00027239 0.00040119 0.00031984 0.0003209  0.00059809\n",
      " 0.00034321 0.00034384 0.00181005 0.00031216 0.00028134 0.0006025\n",
      " 0.00209641 0.00025319 0.00028638 0.00271593 0.0007048  0.00028006\n",
      " 0.00039664 0.00026546 0.00128717 0.00096817 0.0006216  0.00032846\n",
      " 0.00029289 0.01253931 0.00203734 0.00208082 0.01434693 0.00025319\n",
      " 0.00080153 0.00027424 0.00026473 0.00032285 0.00033627 0.00158733\n",
      " 0.00047523 0.0005112  0.00027007 0.00077017 0.00037144 0.00034984\n",
      " 0.00026253 0.00235472 0.00055269 0.00037949 0.00037315 0.00131848\n",
      " 0.00025319 0.0004611  0.00045881 0.00116191 0.00166507 0.001001\n",
      " 0.00137794 0.00025319 0.00036677 0.00032821 0.00025319 0.00176701\n",
      " 0.00095677 0.00185355 0.00156897 0.0002642  0.00042932 0.00110044\n",
      " 0.0004401  0.00029825 0.00028149 0.00032622 0.00040689 0.00073405\n",
      " 0.00093751 0.00318828 0.00053883 0.00117349 0.00038202 0.00046466\n",
      " 0.00032735 0.00045646 0.00028333 0.00038559 0.00043147 0.00026253\n",
      " 0.00025319 0.00095472 0.00098001 0.00260954 0.00097744 0.00030745\n",
      " 0.00080679 0.00514346 0.0002764  0.00055453 0.0003225  0.00032816\n",
      " 0.00034496 0.00035037 0.00108871 0.00030951 0.00027819 0.00087886\n",
      " 0.01524703 0.00026525 0.00077432 0.00129687 0.00087943 0.00059028\n",
      " 0.00037099 0.00057706 0.00040729 0.00050413 0.00026546 0.00065865\n",
      " 0.00602945 0.00037753 0.00028612 0.00042041 0.00094591 0.0013545\n",
      " 0.00025319 0.00105411 0.00050577 0.00137642 0.00076323 0.00035169\n",
      " 0.00108682 0.00083001 0.00058707 0.00380271 0.00042107 0.00037895\n",
      " 0.00028421 0.00984209 0.00169006 0.00057699 0.00031328 0.00025319\n",
      " 0.00486739 0.00043831 0.00179089 0.00027795 0.00030443 0.00757212\n",
      " 0.00026374 0.00028215 0.00124807 0.00038111 0.00027109 0.00084872\n",
      " 0.00048761 0.00116478 0.00177015 0.00079643 0.00034097 0.00112357\n",
      " 0.00340878 0.00197527 0.00026787 0.00372484 0.0003347  0.00133522\n",
      " 0.00028584 0.00026293 0.00030851 0.00038393 0.00066304 0.00025319\n",
      " 0.00048481 0.00025319 0.00385212 0.00044543 0.00071423 0.0002647\n",
      " 0.00228154 0.00025319 0.00063289 0.00028815 0.00058723 0.00063151\n",
      " 0.00047335 0.00106047 0.0015993  0.0009942  0.00134223 0.00045664\n",
      " 0.00065644 0.00084817 0.00028243 0.00036376 0.00813452 0.00025319\n",
      " 0.00725585 0.00031618 0.00234919 0.0003365  0.00025319 0.01033905\n",
      " 0.00088044 0.00050898 0.00025319 0.00038426 0.00043954 0.00166282\n",
      " 0.00031358 0.000398   0.00080533 0.00060169 0.00043544 0.00056137\n",
      " 0.00050761 0.00025319 0.0007274  0.00042807 0.00028392 0.00033434\n",
      " 0.00046863 0.00161012 0.00197678 0.00304861 0.00095576 0.00083283\n",
      " 0.00076544 0.00313636 0.00032094 0.00232665 0.00154883 0.00032913\n",
      " 0.00121063 0.00063513 0.00110504 0.00095904 0.00028298 0.0005719\n",
      " 0.00033799 0.00127897 0.00044615 0.00036355 0.00049598 0.00275747\n",
      " 0.00118915 0.00102913 0.00088602 0.00062441 0.00029223 0.00088552\n",
      " 0.00069274 0.00100497 0.00038105 0.00027699 0.00028996 0.00048256\n",
      " 0.00040071 0.0002642  0.00191767 0.00040051 0.00027187 0.00041142\n",
      " 0.00058479 0.00039718 0.00066365 0.00028897 0.00076263 0.00025319\n",
      " 0.00030513 0.00048811 0.00052461 0.00025319 0.00035785 0.00073776\n",
      " 0.00098891 0.00085393 0.00111169 0.00061544 0.00049443 0.00031017\n",
      " 0.00059764 0.00030068 0.00119924 0.00089807 0.00037084 0.00033458\n",
      " 0.00026808 0.00033683 0.00033645 0.00029625 0.001139   0.00073263\n",
      " 0.00240348 0.00132235 0.00456456 0.0002894  0.00064233 0.00050411\n",
      " 0.00071082 0.00040348 0.00074087 0.00025319 0.00046462 0.00039075\n",
      " 0.0002745  0.00025319 0.00035466 0.00122236 0.00049703 0.00058734\n",
      " 0.0002647  0.00045092 0.00098589 0.00184913 0.00080358 0.00028427\n",
      " 0.00026585 0.00876312 0.00037684 0.0015687  0.00042332 0.00469981\n",
      " 0.00031711 0.00052033 0.00913363 0.00106461 0.00029565 0.00027007\n",
      " 0.0003291  0.00048784 0.00553489 0.00026651 0.00044291 0.00086437\n",
      " 0.00034655 0.00081878 0.00109855 0.00052455 0.0005086  0.00156098\n",
      " 0.00029289 0.00060066 0.00036444 0.00027949 0.00029831 0.00029922\n",
      " 0.00248059 0.00026591 0.001187   0.00046089 0.00030549 0.00036009\n",
      " 0.00378312 0.00046333 0.00151911 0.00047502 0.00025319 0.0003003\n",
      " 0.00040901 0.00144916 0.00037725 0.00108952 0.00037352 0.00049297\n",
      " 0.0014482  0.00038113 0.00094864 0.00027125 0.00916291 0.00082646\n",
      " 0.00033408 0.00029664 0.00283137 0.002058   0.00038826 0.00047832\n",
      " 0.00028496 0.00025319 0.00025319 0.00031797 0.00029532 0.00061736\n",
      " 0.00049831 0.0006012  0.0003046  0.00025319 0.00037329 0.00063856\n",
      " 0.00051533 0.00052825 0.00036316 0.00044543 0.00036696 0.00026725\n",
      " 0.00633838 0.00074405 0.00079717 0.00064864 0.00045377 0.00058409\n",
      " 0.00032306 0.00075822 0.00026304 0.00031919 0.00025319 0.00042726\n",
      " 0.00048977 0.00084882 0.00034688 0.00041723 0.00700113 0.00146806\n",
      " 0.00046793 0.0003194  0.00061969 0.00044246 0.00028837 0.00129356\n",
      " 0.00048814 0.00497992 0.00029159 0.00034857 0.00037047 0.00026747\n",
      " 0.00039777 0.00032886 0.00040124 0.00025319 0.0015916  0.00569145\n",
      " 0.0003374  0.00028111 0.00067515 0.00035364 0.00035102 0.00029197\n",
      " 0.0003328  0.00117812 0.00057196 0.00242662 0.00042144 0.00036719\n",
      " 0.00062557 0.00031379 0.00112442 0.00036828 0.00119384 0.00026323\n",
      " 0.00027826 0.00146085 0.00038345 0.00058311 0.00027149 0.00083237\n",
      " 0.0014209  0.00107055 0.00037863 0.00063015 0.0005751  0.00062571\n",
      " 0.0036676  0.0003487  0.0004215  0.00061512 0.00029887 0.00051363\n",
      " 0.00025319 0.00041165 0.00066842 0.00025319 0.00026901 0.00035791\n",
      " 0.00025319 0.00361617 0.00203314 0.00033755 0.00114355 0.00050374\n",
      " 0.00113073 0.00058985 0.0003913  0.00061517 0.00027007 0.00053664\n",
      " 0.00082063 0.00285941 0.00068652 0.00046718 0.00096076 0.00032577\n",
      " 0.00025319 0.00155318 0.0007619  0.00027458 0.00031639 0.00025319\n",
      " 0.00044275 0.00114792 0.00028871 0.00038818 0.00472887 0.00084772\n",
      " 0.00100357 0.00043189 0.00147676 0.00075497 0.00048993 0.00040838\n",
      " 0.00025319 0.00203263 0.0003394  0.0007785  0.00255972 0.00030549\n",
      " 0.00025319 0.00040397 0.00025319 0.00240185 0.00029092 0.00059133\n",
      " 0.00052235 0.0003018  0.00098849 0.0006989  0.00029505 0.00030981\n",
      " 0.00052597 0.00061047 0.00065739 0.00222569 0.00107368 0.00037625\n",
      " 0.00139546 0.00969769 0.00064709 0.00477429 0.00042538 0.00025319\n",
      " 0.00073919 0.0003193  0.00420752 0.00031723 0.00036247 0.00027752\n",
      " 0.00079077 0.00027007 0.00352432 0.00025319]\n",
      "Ranking order: [401 558 484 347 481 653 245 589 985 820 776 769 418 646 599 648 862 112\n",
      "  54 846 570  78 881 437 782 547 252 364 871 594 987 952 773 187 746  41\n",
      " 191 216 404 992 352 626 585 804 615 912 136 925 220 998 612 309 266 100\n",
      " 529 679 675 392 357 103 937 219 824 695 471 229 132 543 110 964 362 798\n",
      " 336 891 744 969 205 499 650 186 681 255 630  77 378  44 981 391 133 468\n",
      " 483 825 158 482 926 961 224 241  42 674 613 144  61 297 710 517 225 765\n",
      " 164 464 596 608 515 230 590   8 190 508 659 368  91  72 425 673 247  13\n",
      " 638 880 491  17  14 119 518 771 237 791 124 943 682   0 152 806  83 431\n",
      " 956  86 863 901 811 816 221 906  48  34 449 199  10 984 510 579 358  31\n",
      " 575 640  56 617 745 503 231 248 253 351 395 561  66 869 105 476 691 361\n",
      " 602 240 363 759 384 314 684 734 898  25 696 800 446 889 531 438 607 507\n",
      " 308 949 192 928 742 125 930  58 896 611  68 728 143 686 521 788 435  12\n",
      " 813 554 582 441 151 982 321 907 777 375 637 444 577 445 697 416 703 954\n",
      " 509 440 639 339 726 974 764 542 544 280 226 477 940 687 516 676 541 818\n",
      "  39 417 574 317 528 201 350  94 207  62 735 436 448 182 698 701 654 562\n",
      " 557 315  19 785 348 153 727 859 605 643 953 260 111 185 677 905 583 300\n",
      " 821 355 389 936 787 291 131 421 546 662 766 486 386 848 316 609 424 996\n",
      " 117 349 963 140 402 560 290 495 678 262 128 580 277 718 944 324  50 853\n",
      "  37 957 340 429 847   4 752 393 990 725 380 527 743 668 271  32 376 628\n",
      " 750 397 238 472 975 155  46 330 702 242 938   7 204 884 179 920 356 716\n",
      " 622  53  27 569 980 642 313 210 849 986 147  24 748 415 232 839  52 303\n",
      " 685 632 635 388 909 162 370 911 894 699   1 478 169 866 217 833  55 177\n",
      " 729 933 915 276 979 318 101 163 312 467 663 835 175 406 793 433 461 732\n",
      " 127 161 295 971 563 931 138 390 761 634 584 714 851 903  93 565 591 338\n",
      " 298 910 285 890 689 289 382 283 270 259 410 665  96 549 500 412   2 451\n",
      " 197 331 261 327 530 329 102 935 442 405 841 305 249 978 292 326 722 789\n",
      " 972 775 354 156 840 917  22 367 141 399 493 422 365  69 655 790 419 666\n",
      " 578 567 749 929 195 218  79 834 332 760  29 184  92 694  60 730  97 815\n",
      " 198 379 958 858 322 396 870 721 781 606 115 407 624  40 707 167 256 827\n",
      " 453 234 492 807 107   9 636 420 294 243 672 296 864 939 139 189 533 754\n",
      " 145 805 505 801 506 641 535  11 369   3 850 263 763  76 366  71 692 627\n",
      " 843 166 157 784  95 948 867  18 522 337 658 595 664  23 955 538  80 413\n",
      " 520 669 857 988 214 772 301 299 914 892 586 385 573 188 306  16 861 174\n",
      " 134 268 360 121 244  20 919 713  51 810 959 566 526 282  88 342  73 307\n",
      " 967 751 176 878 458 708 711 148  90 359 281 258 661 876 428 715 228 474\n",
      "  47 381 302 251  87 932 755 288 443 826 951 275 196 537 194 657 122 621\n",
      " 902 532 323 817 603 704 414 501 409 587 908 104 571 403 812 770  85 983\n",
      " 274  84  43 286 250 814 838 502 135 496 564 736 183 874 236 325 455 897\n",
      " 893 844 512 794 645 693 842 994 142 171 803 106 279 923 724 265 150  64\n",
      "  30 758 885 374 118 581 239 886 345 553 200 497 267 913 873 310 430 222\n",
      " 860 786 398 334 552  45 463 462  98 333 203 610 962 215 304 690 114 180\n",
      " 927 882 739 651 740 490 154 353 287 616 737 671 822 269 888 246 146  38\n",
      " 683 780 877 479 513 551  63 534 172 346 525 941 116 113 343 852 489 173\n",
      " 550 680 460 120 272 459 456 865 991 855 129 831  35 993 774 335 165 946\n",
      " 423 649  99 434 895 660 592 130 465 193 731 977 555 432 377 319 620  57\n",
      "  75 545 206 213 965 802 720 836 598 293 973 126  28  65 733 223 809 211\n",
      " 797 233 916 796 523 439 264 823  33 212 741 778 832 976 278 411 202  36\n",
      " 480 792 700 887 400 872 970 227 209 706 454 257 747 149 717 137 950 868\n",
      " 633 470 572 383 618 273 828 767 588 670 447 536 688 644 601 524 466 181\n",
      " 883 108 473 284 387 795 168 900 556 597 995 344 705 548 945 756 487 373\n",
      " 457 712 904 311  67   6 819 604 494 779 997 934 922  26 738 614 178 875\n",
      " 845  89 783 371 799  15 768 208 170 568 475 254 559  49 488 762  81 629\n",
      " 519 709 394 600 899 854 619 539 498 837 856  82 966 960 968 999 109 511\n",
      " 753 235 328 450 625 921 623 719 427 514 123 372 829 942 652 159 808 469\n",
      " 631  59 647 593 656 757  70 667  74 947 830 504 341   5 879 160 408 540\n",
      " 320 918 452 426 485 576 924 723  21 989]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse.linalg import eigs\n",
    "\n",
    "def rank_centrality_from_matrix(W):\n",
    "    \"\"\"\n",
    "    Compute ranking using Rank Centrality from a win count matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - W: numpy array of shape (n_items, n_items),\n",
    "         where W[i, j] = number of times item i beat item j\n",
    "\n",
    "    Returns:\n",
    "    - scores: stationary distribution of the Markov chain (ranking scores)\n",
    "    - ranking: sorted item indices in descending order of scores\n",
    "    \"\"\"\n",
    "    n = W.shape[0]\n",
    "    P = np.zeros((n, n))\n",
    "\n",
    "    # Build transition matrix\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                wins_i = W[i, j]\n",
    "                wins_j = W[j, i]\n",
    "                total = wins_i + wins_j\n",
    "                if total > 0:\n",
    "                    P[i, j] = wins_j / total  # Probability of moving to j (more wins of j means i \"loses\" to j)\n",
    "\n",
    "    # Normalize rows and add self-loops to make stochastic\n",
    "    for i in range(n):\n",
    "        row_sum = np.sum(P[i, :])\n",
    "        if row_sum > 0:\n",
    "            P[i, :] /= row_sum\n",
    "        else:\n",
    "            P[i, :] = 1.0 / n  # uniform if no comparisons\n",
    "        P[i, i] = 1.0 - np.sum(P[i, :]) + P[i, i]  # ensure stochasticity\n",
    "\n",
    "    # Compute stationary distribution (left eigenvector of transpose)\n",
    "    vals, vecs = eigs(P.T, k=1, which='LM')\n",
    "    stationary = np.real(vecs[:, 0])\n",
    "    stationary = stationary / np.sum(stationary)\n",
    "\n",
    "    ranking = np.argsort(-stationary)\n",
    "\n",
    "    return stationary, ranking\n",
    "\n",
    "# Example use:\n",
    "# W[i, j] = number of times item i beat item j\n",
    "\n",
    "\n",
    "scores, ranking = rank_centrality_from_matrix(a)\n",
    "print(\"Ranking scores:\", scores)\n",
    "print(\"Ranking order:\", ranking)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1842d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_coherence_score(batch[558])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
